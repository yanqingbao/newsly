## Datasets
### The following datasets are a good starting place for summarization tasks.  

Most are also used in standard NLP benchmarking. These are the same resources found in my original README.

I've also included the BBC News dataset in this directory. It's a quality, compact, and manageable dataset which includes full-text articles along with their accompanying "gold" summaries. And it's small enough in size to fit into this repo ;-)

### The Rest:
 - [CNN/DailyMail](https://github.com/abisee/cnn-dailymail): A very common dataset for NLP summarization. Includes both full text articles and their accompanying reference (gold) summaries. This is a large dataset. Instructions for loading preprocessed data or DIY options are available.
 - [Webis-TLDR-17 Corpus](https://zenodo.org/record/1168855): A more recent and very interesting large dataset. It was created by mining self-written summaries on Reddit marked by users' comments who appended posts with a literal TL;DR.
 - [Gigaword](https://www.ldc.upenn.edu/language-resources): Another common dataset for summarization in part because it constitutes one of the largest of the corpora. Plus, there are multiple editions. But--and it's a big but (sorry)--you've got to pay for it, and it's not cheap. Because it's so widely used, I'd wager there are copies of the dataset floating around the web, but as usual it's on you to read the fine print.
 - [Opinosis](https://github.com/kavgan/opinosis/blob/master/OpinosisDataset1.0_0.zip): A useful dataset for summarization of reviews and complete with gold summaries. A smaller dataset, it's best for those who don't have a ton of compute to spare. It's organized around topics (e.g., "performance of Toyota Camry") and the reviews within those topics. It was released alongside a summarization model, so you might also want to check that out. Link goes directly to a zip file.
 - [WikiHow](https://github.com/mahnazkoupaee/WikiHow-Dataset): The dataset author explains it best: "Each article consists of multiple paragraphs and each paragraph starts with a sentence summarizing it. By merging the paragraphs to form the article and the paragraph outlines to form the summary, the resulting version of the dataset contains more than 200,000 long-sequence pairs." This is considered one of the largest datasets for summarization. I found the setup to be somewhat awkward/confounding, but after that you're good to go.
 - Other:  
  - [Kaggle](https://www.kaggle.com) is always a good place to look. I've tried their [New York Times](https://www.kaggle.com/nzalake52/new-york-times-articles) article dataset and it's where I found the BBC News dataset.  
  - While the NYT dataset doesn't come with gold summaries, it's fairly common to use a news dataset to predict the headline of an article (especially when gold summaries are unavailable).  
  - Also, you might find it worthwhile to visit the relatively new [TensorFlow Datasets](https://www.tensorflow.org/datasets) site. I've only skimmed their offerings, but I do know they have the CNN/DailyMail dataset (with additional options) among others. The real benefit is that they provide very easy code to import any of their datasets through a `tfds.load()` helper function. For example, this could be a more efficient way to load in the unwieldy CNN/DailyMail dataset. Give it a try!
  - Lastly, the [NLP Progress](http://nlpprogress.com/english/summarization.html) Summarization section and [Papers with Code](https://www.paperswithcode.com) are great resources to see which datasets and high-performance models are currently utilized by those at the top of their game. Fair warning, Papers with Code encompasses all of deep learning--not just NLP, so it's super easy to get distracted there with so many novel ideas.